{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Deep learning for computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to convnets\n",
    "\n",
    "In this section, we are going to take a look at convolutional networks and talk about why they have been so sucessful at computer vision tasks. To get started, we are going to see how we can improve the digit recognition from the MNIST dataset from chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difference\n",
    "\n",
    "The section below is where we start to depart from the example from chapter 2. Instead of a network consisting of just densely connected layers, there is a layer of Conv2D layers that I like to think of as a pre-processing step for the densely connected layers. Convolutional layers excell at finding patterns in images because they apply a set of filters over the entire image where dense layers apply their filters indescriminately. I know this is probably a bad example but it's how I like to think about it.\n",
    "\n",
    "As we can see in the example below, Maxpooling2D effectifely halves the size of the netweek each time it is called. This is be design as MaxPooling is intended to aggresively downsample the feature maps from the preceding Conv2D layer. The MaxPooling operation is similar to the Convolution operation except it does a max tensor operation with a stride of 2 instead of 3, hence the halving. This downsampling decreases the total amount of tensors in a network thereby decreasing the overfitting of the model. The downsampling helps the model generalize features by looking at larger windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional methods\n",
    "\n",
    "After the convolutional layers we add two densely connected layers. At first, this did not make sense to me. But then I considered what each layer type excells at. Convolutional layers excell at finding image features and converting the image (2D tensor) to a list of probabilities that each feature exists. Dense layers excell at finding correlations in numerical data. So this network is doing feature extraction on the image using convolutional layers and then doing classification on the resulting feature probabilities. Smart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data\n",
    "\n",
    "Again we have an example of data normalization but in this case its an image so normalization is pretty straighforward because we know the bounds of the numbers inside. First we cast all of the values contained in the image arrays to floats (to preserve precision after normalization) and then we divide by 255 to map all values to a value between 0 to 1. Next we see to_categorical being used to set test and train labels. This basically converts the result integer into a binary array that can be understood by the algorithm. \n",
    "\n",
    "For example a label array containing:  \n",
    "`[1, 3]`   \n",
    "would be converted to:   \n",
    "`[[0, 1, 0, 0], \n",
    " [0, 0, 0, 1]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.1650 - accuracy: 0.9471\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0469 - accuracy: 0.9854\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0324 - accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0254 - accuracy: 0.9919\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0195 - accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f23480b3940>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After compiling and training the model we see that just by including the convolutional layers we have increased the accuracy of the model to 99% versus the 97% we got in chapter two. A marked improvement from smarter model architecture versus trying to brute force improvements with more data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 62us/sample - loss: 0.0346 - accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a convnet using a small datasets\n",
    "\n",
    "In this example, we will be using a convnet and data augmentation to train a convolutional model with a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = '../datasets/dogs-vs-cats'\n",
    "base_dir = '../datasets/dogs-vs-cats-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyImages(source, destination, irange):    \n",
    "    dest_cat_dir = os.path.join(destination, 'cats')\n",
    "    dest_dog_dir = os.path.join(destination, 'dogs')\n",
    "    \n",
    "    cfnames = ['cat.{}.jpg'.format(i) for i in irange]\n",
    "    for cfname in cfnames:\n",
    "        src = os.path.join(source, cfname)\n",
    "        dst = os.path.join(dest_cat_dir, cfname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    dfnames = ['dog.{}.jpg'.format(i) for i in irange]\n",
    "    for dfname in dfnames:\n",
    "        src = os.path.join(source, dfname)\n",
    "        dst = os.path.join(dest_dog_dir, dfname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images:  1000\n",
      "total training dog images:  1000\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(train_dir)\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "original_train_dir = os.path.join(original_dataset_dir, 'train')\n",
    "\n",
    "copyImages(original_train_dir, train_dir, range(1000))\n",
    "\n",
    "print('total training cat images: ', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images: ', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images:  500\n",
      "total validation dog images:  500\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(validation_dir)\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "original_train_dir = os.path.join(original_dataset_dir, 'train')\n",
    "\n",
    "copyImages(original_train_dir, validation_dir, range(1000, 1500))\n",
    "\n",
    "print('total validation cat images: ', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images: ', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images:  500\n",
      "total test dog images:  500\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(test_dir)\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "original_train_dir = os.path.join(original_dataset_dir, 'train')\n",
    "\n",
    "copyImages(original_train_dir, test_dir, range(1500, 2000))\n",
    "\n",
    "print('total test cat images: ', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images: ', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 100 steps, validate for 50 steps\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.6920 - acc: 0.5380 - val_loss: 0.6782 - val_acc: 0.5150\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6701 - acc: 0.5960 - val_loss: 0.6590 - val_acc: 0.5930\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6349 - acc: 0.6455 - val_loss: 0.6151 - val_acc: 0.6710\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5813 - acc: 0.6940 - val_loss: 0.6232 - val_acc: 0.6430\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5425 - acc: 0.7240 - val_loss: 0.5971 - val_acc: 0.6720\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5044 - acc: 0.7500 - val_loss: 0.5590 - val_acc: 0.7200\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.4783 - acc: 0.7745 - val_loss: 0.5551 - val_acc: 0.7100\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.4539 - acc: 0.7815 - val_loss: 0.5405 - val_acc: 0.7320\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.4117 - acc: 0.8120 - val_loss: 0.5423 - val_acc: 0.7360\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.3994 - acc: 0.8205 - val_loss: 0.5730 - val_acc: 0.7190\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.3745 - acc: 0.8360 - val_loss: 0.5937 - val_acc: 0.7220\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.3547 - acc: 0.8500 - val_loss: 0.5586 - val_acc: 0.7360\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.3284 - acc: 0.8590 - val_loss: 0.5551 - val_acc: 0.7400\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.3051 - acc: 0.8665 - val_loss: 0.5704 - val_acc: 0.7210\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.2800 - acc: 0.8840 - val_loss: 0.5841 - val_acc: 0.7340\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.2552 - acc: 0.9030 - val_loss: 0.6034 - val_acc: 0.7290\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.2275 - acc: 0.9065 - val_loss: 0.6163 - val_acc: 0.7310\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.2115 - acc: 0.9160 - val_loss: 0.7027 - val_acc: 0.7010\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.1904 - acc: 0.9290 - val_loss: 0.6519 - val_acc: 0.7240\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.1777 - acc: 0.9385 - val_loss: 0.6622 - val_acc: 0.7310\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.1488 - acc: 0.9505 - val_loss: 0.7767 - val_acc: 0.7250\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.1388 - acc: 0.9495 - val_loss: 0.7568 - val_acc: 0.7240\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.1150 - acc: 0.9665 - val_loss: 0.7972 - val_acc: 0.7190\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.1016 - acc: 0.9690 - val_loss: 0.8203 - val_acc: 0.7210\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.0826 - acc: 0.9785 - val_loss: 0.8561 - val_acc: 0.7270\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.0749 - acc: 0.9775 - val_loss: 0.8787 - val_acc: 0.7270\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.0637 - acc: 0.9810 - val_loss: 0.9734 - val_acc: 0.7100\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.0604 - acc: 0.9830 - val_loss: 1.0169 - val_acc: 0.7200\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.0465 - acc: 0.9855 - val_loss: 0.9654 - val_acc: 0.7270\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.0395 - acc: 0.9895 - val_loss: 1.0273 - val_acc: 0.7220\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                   steps_per_epoch=100,\n",
    "                   epochs=30,\n",
    "                   validation_data=validation_generator,\n",
    "                   validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
